---
title: "TE2021"
subtitle: "Problema 3"
author: "Mauro Esteban Lioy"
output: html_notebook
---

<style type="text/css">
.main-container {
  max-width: 700px;
  margin-left: auto;
  margin-right: auto;
}
</style>

# Problema3.2
--------------------------------------------------------------------------------
Para cada uno de los 10 dígitos (0,...,9) se tienen 200 imágenes digitalizadas del dígito escrito a mano. De cada imagen se obtuvieron 649 características (“features”).  Se trata de predecir el digito correspondiente a una imagen, en función de sus características. 
	Los datos están repartidos en 6 archivos, todos  con m=2000, cada uno de los cuales corresponde a un tipo de características: mfeat-fou, -fac, -kar, -px-, zer-. mor. De modo que luego hay que “pegarlos”.
	Las primeras 200 filas de cada archivo corresponden a la clase “0”, las siguientes 200 a “1”. etc.
	Aplique los métodos que le parezcan convenientes y compare sus performances.
	Los datos están en
http://archive.ics.uci.edu/ml/datasets/Multiple+Features


    1. mfeat-fou: 76 Fourier coefficients of the character shapes;
    2. mfeat-fac: 216 profile correlations;
    3. mfeat-kar: 64 Karhunen-Love coefficients;
    4. mfeat-pix: 240 pixel averages in 2 x 3 windows;
    5. mfeat-zer: 47 Zernike moments;
    6. mfeat-mor: 6 morphological features.

--------------------------------------------------------------------------------
```{r, echo=FALSE,warning=FALSE,include = FALSE}
rm(list = ls())
graphics.off()

library(ggplot2)
library(leaps)
library(broom)
library(RobStatTM)
library(reshape2)
library(stringr)
```

## DATOS

```{r}
fou<-read.table("mfeat-fou",header=FALSE)
fac<-read.table("mfeat-fac",header=FALSE)
kar<-read.table("mfeat-kar",header=FALSE)
pix<-read.table("mfeat-pix",header=FALSE)
zer<-read.table("mfeat-zer",header=FALSE)
mor<-read.table("mfeat-mor",header=FALSE)

mor$V1<-as.factor(mor$V1)
mor$V2<-as.factor(mor$V2)
mor$V3<-as.factor(mor$V3)


mfeat<-cbind(fou,fac,kar,pix,zer,mor)
colnames(mfeat)<-as.vector(seq(1,649,1))
class<-rep(0,2000)
class[(1:200)]<-0
class[(201:400)]<-1
class[(401:600)]<-2
class[(601:800)]<-3
class[(801:1000)]<-4
class[(1001:1200)]<-5
class[(1201:1400)]<-6
class[(1401:1600)]<-7
class[(1601:1800)]<-8
class[(1801:2000)]<-9
class<-as.factor(class)
datos<-cbind(mfeat,class)
```



## DESCRIPTIVO DE LOS DATOS
--------------------------------------------------------------------------------
```{r}
table(datos$class)
```
```{r}
str(mor)
```

```{r}
str(datos)
```



(hacer descriptivo)


--------------------------------------------------------------------------------

## TRAIN-TEST

```{r}
set.seed(100)
n = nrow(datos)
trainIndex = sample(1:n, size = round(0.7*n), replace=FALSE)
ds.train = datos[trainIndex ,]
ds.test = datos[-trainIndex ,]
```

```{r}
table(ds.train$class)
table(ds.test$class)
```


--------------------------------------------------------------------------------



### Arbol rpart
```{r}
library(rpart)
model_tree <- rpart( class ~ ., data=ds.train, method = "class", control = rpart.control(minsplit = 5, cp = 0., xval = 5, maxdepth = 30)) 
summary(model_tree)
#plot(model_tree)
#text(model_tree)
```


```{r}
pred_train = predict(model_tree, ds.train, type="class")
pred_test = predict(model_tree, ds.test, type="class")
```

```{r}
#error de clasificación 
mean(ifelse(ds.train$class==pred_train,1,0)) 
mean(ifelse(ds.test$class==pred_test,1,0)) 
```

```{r}
model_tree_p = prune(model_tree, cp=0.01)
```

```{r}
pred_train = predict(model_tree_p, ds.train, type="class")
pred_test = predict(model_tree_p, ds.test, type="class")
```

```{r}
#error de clasificación 
mean(ifelse(ds.train$class==pred_train,1,0)) 
mean(ifelse(ds.test$class==pred_test,1,0)) 
```

```{r}
ds.train.n = setNames(ds.train, c(paste0("V", 1:649), "class"))
ds.test.n = setNames(ds.test, c(paste0("V", 1:649), "class"))
```
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------


## Arbol tree 

```{r}
library (tree)
set.seed(100)
model_tree_2  <- tree(formula = ds.train$class ~.,ds.train.n, method = "class")#,control = tree.control(nobs=2000,minsize = 1 ))
```

```{r}
summary(model_tree_2)
```

```{r}
plot(model_tree_2)
text(model_tree_2)
```

```{r}
pred_train = predict(model_tree_2, ds.train.n, type="class")
pred_test = predict(model_tree_2, ds.test.n, type="class")
```

```{r}
#error de clasificación 
mean(ifelse(ds.train.n$class==pred_train,1,0)) 
mean(ifelse(ds.test.n$class==pred_test,1,0)) 
```

## Arbol cv.tree

```{r}
set.seed(100)
model_tree_2_cv = cv.tree(model_tree_2, FUN=prune.misclass, K=5  )
```

```{r}
model_tree_2_cv
```

```{r}
par ( mfrow =c(1 ,2) )
plot(model_tree_2_cv$size,model_tree_2_cv$dev ,type ="b")
plot(model_tree_2_cv$k ,model_tree_2_cv$dev ,type ="b")
```

```{r}
set.seed(100)
model_tree_2_cv_poda <- prune.misclass(model_tree_2, best=13)
summary(model_tree_2_cv_poda)
```

```{r}
set.seed(100)
pred_train_m2cvpoda = predict(model_tree_2_cv_poda, ds.train.n, type="class")
pred_test_m2cvpoda = predict(model_tree_2_cv_poda, ds.test.n, type="class")
#error de clasificación 
mean(ifelse(ds.train.n$class==pred_train_m2cvpoda,1,0)) 
mean(ifelse(ds.test.n$class==pred_test_m2cvpoda,1,0)) 
```
```{r}
#png(filename = "arbol_cv.png", width = 800, height = 600)
plot(model_tree_2_cv_poda)
text(model_tree_2_cv_poda,pretty =0,cex=0.7)
#dev.off()
```



```{r}
(mc <- with(ds.test.n,table(pred_test_m2cvpoda, class)))
```


-------------------------------------------------------------------------------


-------------------------------------------------------------------------------

## RandomForest

```{r}
library(randomForest)
set.seed(100)
model_bagg <- randomForest(class~.,ds.train.n, importance=TRUE)
                           
                           #xtest = ds.test.n,
                           #ytest = ds.test.n$class)
model_bagg
```


```{r}
model_bagg$localImportance
```

```{r}
pred_train_rf = predict(model_bagg, ds.train.n, type="class")
pred_test_rf = predict(model_bagg, ds.test.n, type="class")
#error de clasificación 
mean(ifelse(ds.train.n$class==pred_train_rf,1,0)) 
mean(ifelse(ds.test.n$class==pred_test_rf,1,0)) 
```
```{r}
(mc <- with(ds.test.n,table(pred_test_rf, class)))
```

--------------------------------------------------------------------------------
--------------------------------------------------------------------------------


## Regresión Logística Multiclase 

```{r}
library(glmnet)
```


```{r}
set.seed(110)

XX<-model.matrix(lm(class~.,data=ds.train))
Xtest<-model.matrix(lm(class~.,data=ds.test))

cv.lasso<-cv.glmnet(x=XX,y=ds.train$class,family="multinomial",alpha=1,nfolds=3,trace.it=1)
ajustelasso<-glmnet(x=XX,y=ds.train$class,family="multinomial",alpha=1,lambda=cv.lasso$lambda.1se, trace.it=1)
mean(predict(ajustelasso,newx=Xtest,type="class")==ds.test$class)
cnf <- confusion.glmnet(ajustelasso, newx = Xtest, newy = ds.test$class)
```

```{r}
cnf
```

```{r}
pred.lasso.multi = predict(ajustelasso, newx = Xtest, newy = ds.test$class, type = "class")
```


```{r}
#error de clasificación 
mean(predict(ajustelasso,newx=XX,type="class")==ds.train$class)
mean(ifelse(ds.test.n$class!=pred.lasso.multi,1,0))
mean(ifelse(ds.test.n$class==pred.lasso.multi,1,0)) 
```


```{r}
# RIDGE
#XX<-model.matrix(lm(class~.,data=ds.train))
#Xtest<-model.matrix(lm(class~.,data=ds.test))
cv.ridge<-cv.glmnet(x=XX,y=ds.train$class,family="multinomial",alpha=0,nfolds=3,)
ajusteridge<-glmnet(x=XX,y=ds.train$class,family="multinomial",alpha=0,lambda=cv.ridge$lambda.1se)
mean(predict(ajusteridge,newx=Xtest,type="class")==ds.test$class)
cnf <- confusion.glmnet(ajusteridge, newx = Xtest, newy = ds.test$class)
```

```{r}
cnf
```

```{r}
pred.ridge.multi = predict(ajusteridge, newx = Xtest, newy = ds.test$class, type = "class")
```

```{r}
#error de clasificación 
mean(predict(ajusteridge,newx=XX,type="class")==ds.train$class)
mean(ifelse(ds.test.n$class!=pred.ridge.multi,1,0))
mean(ifelse(ds.test.n$class==pred.ridge.multi,1,0)) 
```







```{r}
metricasMulti = function(class_orig,class_pred,vector_class){
  kk=c()
  
  for(k in vector_class){
    
    aux = ifelse(class_orig==k,1,0)
    aux_pred = ifelse(class_pred==k,1,0)
    Nk = sum(aux)
    
    TPk = sum(ifelse(aux==1 & aux_pred==1,1,0)) 
    TNk = sum(ifelse(aux==0 & aux_pred==0,1,0))
    FPk = sum(ifelse(aux==0 & aux_pred==1,1,0))
    FNk = sum(ifelse(aux==1 & aux_pred==0,1,0))

    accuracy_k =  (TPk + TNk) / (TPk+TNk+FPk+FNk)
    precision_k =  TPk / (TPk+FPk)
    recall_k = TPk / (TPk+FNk)
    F1_k = TPk / (TPk + 0.5*(FPk+FNk))
    
    kk <- rbind(kk,c(k,round(accuracy_k,3),round(precision_k,3),round(recall_k,3),round(F1_k,3),TPk,TNk,FPk,FNk,Nk))
  }
  
  
  ds.metrics = data.frame(kk)
  names(ds.metrics) <- c("class","accuracy","precision","recall","F1","TP","TN","FP","FN","Nk")
  ds.metrics = rbind(ds.metrics, apply(ds.metrics, FUN="mean", MARGIN = 2))
  ds.metrics[11,1] = "mean"
  return(ds.metrics)
  }
```

```{r}
vector_class = c(0,1,2,3,4,5,6,7,8,9)
metrics_multi = metricasMulti(ds.test.n$class,pred.lasso.multi,vector_class = vector_class)
```


```{r}
metrics_multi
```

```{r}
prod(metrics_multi[,2][1:10])
```

```{r}
vector_class = c(0,1,2,3,4,5,6,7,8,9)
metrics_multi = metricasMulti(ds.test.n$class,pred.ridge.multi,vector_class = vector_class)
```


```{r}
metrics_multi
```


```{r}
prod(metrics_multi[,2][1:10])
```





```{r}
vector_class = c(0,1,2,3,4,5,6,7,8,9)
metricasMulti(ds.test.n$class,pred_test_m2cvpoda,vector_class = vector_class)
```



```{r}
vector_class = c(0,1,2,3,4,5,6,7,8,9)
metricasMulti(ds.test.n$class,pred_test_rf,vector_class = vector_class)
```






--------------------------------------------------




## METRICAS ROC
```{r}
library(ROCR)
```



```{r}
#ejemplo https://rpubs.com/JairoAyala/592802

pred <- prediction(ds.train$intensityMean, ds.train$class1)
perf <- performance(pred,measure="tpr",x.measure="fpr")


plot(perf,colorize=TRUE,type="l") 
abline(a=0,b=1)

# Área bajo la curva
AUC       <- performance(pred,measure="auc")
AUCaltura <- AUC@y.values

# Punto de corte óptimo
cost.perf <- performance(pred, measure ="cost")
opt.cut   <- pred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]
#coordenadas del punto de corte óptimo
x<-perf@x.values[[1]][which.min(cost.perf@y.values[[1]])]
y<-perf@y.values[[1]][which.min(cost.perf@y.values[[1]])]
points(x,y, pch=20, col="red")

```

--------------------------------------------------------------------------------

```{r}
sum(pred.lasso.multi[1,,1])
```
#PCA

```{r PCArapido,echo=FALSE}
library(factoextra)
#Hago el PCA
res.pca <- prcomp(as.matrix(ds.train[,1:640]), scale = F)
#Variación versus PC
plot(res.pca, main="Varación en función del número de PC")
#Primeras dos PC
```
```{r}
dim(res.pca$x)
```


```{r}
plot(res.pca$x[,1],res.pca$x[,2])
```


```{r}
model_tree_pca <- rpart( ds.train$class ~ ., data=as.data.frame(res.pca$x), method = "class", control = rpart.control(minsplit = 5, cp = 0., xval = 5, maxdepth = 30)) 
summary(model_tree_pca)
```


```{r}
plot(model_tree_pca)
text(model_tree_pca)
```



```{r}

ds.train.pca =  as.data.frame(as.matrix(ds.train[,1:640])%*%res.pca$rotation)
ds.test.pca =  as.data.frame(as.matrix(ds.test[,1:640])%*%res.pca$rotation)


pred_train = predict(model_tree_pca, ds.train.pca, type="class")
pred_test = predict(model_tree_pca, ds.test.pca, type="class")
#error de clasificación 
mean(ifelse(ds.train.n$class==pred_train,1,0)) 
mean(ifelse(ds.test.n$class==pred_test,1,0)) 
```
